{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ad9d5e-fc74-4b82-9134-89ad9b9b781c",
   "metadata": {},
   "source": [
    "What is LdaMulticore in Gensim?\n",
    "LdaMulticore is a parallelized version of LdaModel in Gensim that trains Latent Dirichlet Allocation (LDA) topic models faster using multiple CPU cores.\n",
    "\n",
    "It‚Äôs ideal for large text corpora where training with LdaModel would be too slow.\n",
    "\n",
    "‚úÖ When to Use:\n",
    "You have a large number of documents (thousands+)\n",
    "\n",
    "You want to speed up topic modeling\n",
    "\n",
    "You have a multi-core CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0287fc3c-3041-40eb-9aff-ff16b9d43ebc",
   "metadata": {},
   "source": [
    "8.2 Topic Modelling using LDA\n",
    "LDA is a popular method for topic modelling which considers each document as a collection of topics in a certain proportion. We need to take out the good quality of topics such as how segregated and meaningful they are. The good quality topics depend on- \n",
    "\n",
    "The quality of text processing\n",
    "Finding the optimal number of topics\n",
    "Tuning parameters of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079941f-15c9-44f0-b106-a01e96f4d76a",
   "metadata": {},
   "source": [
    "Prepare the Data \n",
    "This is done by removing the stopwords and then lemmatizing it. In order to lemmatize using Gensim, we need to first download the pattern package and the stopwords. \n",
    "Let's install pattern package and import nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea62f37a-5d11-4582-b6e1-db3a722f034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Using cached pattern-0.0.1a0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: seaborn>=0.10.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from pattern) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.4 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from pattern) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=1.5.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from pattern) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.9.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from pattern) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from matplotlib>=3.9.0->pattern) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from scikit-learn>=1.5.0->pattern) (3.5.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from seaborn>=0.10.0->pattern) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn>=0.10.0->pattern) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn>=0.10.0->pattern) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\welcome\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.9.0->pattern) (1.16.0)\n",
      "Using cached pattern-0.0.1a0-py3-none-any.whl (4.0 kB)\n",
      "Installing collected packages: pattern\n",
      "Successfully installed pattern-0.0.1a0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad22c09-6f89-47bc-8b6c-c6307fadc90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7d262-7d18-45d7-9ea9-02a34ab58800",
   "metadata": {},
   "source": [
    "we have pre-processed the data by removing stopwords and lemmatization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac486419-b9ee-450e-bc3e-f987828fe89c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'lemmatize' from 'gensim.utils' (C:\\Users\\Welcome\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LdaModel, LdaMulticore\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess, lemmatize\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'lemmatize' from 'gensim.utils' (C:\\Users\\Welcome\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\utils.py)"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format ='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(level = logging.INFO)\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['subject', 'com', 'are', 'edu', 'would', 'could']\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "data = [w for w in dataset]\n",
    "\n",
    "# Preparing the data\n",
    "processed_data = []\n",
    "for x, doc in enumerate(data[:100]):\n",
    "    doc_out = []\n",
    "    for word in doc:\n",
    "        if word not in stop_words: \n",
    "            Lemmatized_Word = lemmatize(word, allowed_tags = re.compile('(NN|JJ|RB)'))  # lemmatize\n",
    "            if Lemmatized_Word:\n",
    "                doc_out.append(Lemmatized_Word[0].split(b'/')[0].decode('utf-8'))\n",
    "        else:\n",
    "            continue\n",
    "    processed_data.append(doc_out) \n",
    "print(processed_data[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317699bc-86af-4bce-b95c-1f2a6bcec857",
   "metadata": {},
   "source": [
    "You're seeing this error:\n",
    "\n",
    "\n",
    "ImportError: cannot import name 'lemmatize' from 'gensim.utils'\n",
    "‚ùå Problem:\n",
    "lemmatize is not part of gensim.utils anymore ‚Äî it was removed from recent versions of Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b09309-ecd9-4bf3-a2fa-d02cea3d27fa",
   "metadata": {},
   "source": [
    "Use spaCy for lemmatization instead.\n",
    "Let me show you how to correctly preprocess and lemmatize your data using spaCy, keeping everything else the same.\n",
    "\n",
    "‚úÖ Step-by-Step Fix with spaCy\n",
    "1. üì¶ Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e049f0-759f-4704-80ea-c5115f051688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\welcome\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 6.3/14.9 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 40.9 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp310-cp310-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp310-cp310-win_amd64.whl (117 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 36.0 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.3/632.3 kB 23.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 48.8 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp310-cp310-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 47.9 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.4/5.4 MB 46.6 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\n",
      "   ---- -----------------------------------  3/27 [spacy-loggers]\n",
      "   ------- --------------------------------  5/27 [shellingham]\n",
      "   -------- -------------------------------  6/27 [pydantic-core]\n",
      "  Attempting uninstall: numpy\n",
      "   -------- -------------------------------  6/27 [pydantic-core]\n",
      "    Found existing installation: numpy 1.26.4\n",
      "   -------- -------------------------------  6/27 [pydantic-core]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------- -----------------------------  7/27 [numpy]\n",
      "   ---------------- ----------------------- 11/27 [cloudpathlib]\n",
      "   ----------------- ---------------------- 12/27 [catalogue]\n",
      "   -------------------- ------------------- 14/27 [srsly]\n",
      "   -------------------- ------------------- 14/27 [srsly]\n",
      "   -------------------- ------------------- 14/27 [srsly]\n",
      "   -------------------- ------------------- 14/27 [srsly]\n",
      "   ---------------------- ----------------- 15/27 [pydantic]\n",
      "   ---------------------- ----------------- 15/27 [pydantic]\n",
      "   ---------------------- ----------------- 15/27 [pydantic]\n",
      "   ---------------------- ----------------- 15/27 [pydantic]\n",
      "   ------------------------- -------------- 17/27 [markdown-it-py]\n",
      "   ------------------------- -------------- 17/27 [markdown-it-py]\n",
      "   ------------------------- -------------- 17/27 [markdown-it-py]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   -------------------------- ------------- 18/27 [language-data]\n",
      "   ---------------------------- ----------- 19/27 [blis]\n",
      "   ---------------------------- ----------- 19/27 [blis]\n",
      "   ----------------------------- ---------- 20/27 [rich]\n",
      "   ----------------------------- ---------- 20/27 [rich]\n",
      "   ----------------------------- ---------- 20/27 [rich]\n",
      "   -------------------------------- ------- 22/27 [confection]\n",
      "   ----------------------------------- ---- 24/27 [thinc]\n",
      "   ----------------------------------- ---- 24/27 [thinc]\n",
      "   ----------------------------------- ---- 24/27 [thinc]\n",
      "   ----------------------------------- ---- 24/27 [thinc]\n",
      "   ----------------------------------- ---- 24/27 [thinc]\n",
      "   ----------------------------------- ---- 24/27 [thinc]\n",
      "   ------------------------------------- -- 25/27 [weasel]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   -------------------------------------- - 26/27 [spacy]\n",
      "   ---------------------------------------- 27/27 [spacy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Welcome\\anaconda3\\envs\\myenv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Welcome\\anaconda3\\envs\\myenv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.2.6 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 45.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 44.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf8d4-295e-418a-8c15-3da87ea6b16e",
   "metadata": {},
   "source": [
    "Update your preprocessing code:\n",
    "Replace the old lemmatize(...) logic with this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd091c0-8b5f-4099-873f-e78b722fa729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Dataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Group words into pseudo-documents\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m docs \u001b[38;5;241m=\u001b[39m [data[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m100\u001b[39m)]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Load spaCy model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m data \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Group words into pseudo-documents\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m100\u001b[39m)]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Load spaCy model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Dataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') + ['subject', 'com', 'edu', 'would', 'could']\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Load text8 data\n",
    "dataset = api.load(\"text8\")\n",
    "data = dataset\n",
    "\n",
    "# Group words into pseudo-documents\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess with lemmatization\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        tokens = nlp(\" \".join(doc))\n",
    "        processed_doc = [token.lemma_ for token in tokens \n",
    "                         if not token.is_stop and token.is_alpha and token.lemma_ not in stop_words]\n",
    "        processed.append(processed_doc)\n",
    "    return processed\n",
    "\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "print(processed_data[0][:10])  # Show first 10 lemmatized words of the first doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae639e0f-9947-4a95-9549-90bdd68aea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Run preprocessing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_lemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_data[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m, in \u001b[0;36mspacy_lemmatize\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     28\u001b[0m processed \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m---> 30\u001b[0m     joined \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Now doc is a list of strings\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nlp(joined)\n\u001b[0;32m     32\u001b[0m     processed_doc \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \n\u001b[0;32m     33\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_alpha \u001b[38;5;129;01mand\u001b[39;00m token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') + ['subject', 'com', 'edu', 'would', 'could']\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Load text8 data\n",
    "dataset = api.load(\"text8\")  # Already tokenized as one big list of words\n",
    "data = list(dataset)  # DO NOT re-wrap it\n",
    "\n",
    "# Group words into pseudo-documents\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]  # Each doc is 100 words\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess with lemmatization\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        joined = \" \".join(doc)  # Now doc is a list of strings\n",
    "        tokens = nlp(joined)\n",
    "        processed_doc = [token.lemma_ for token in tokens \n",
    "                         if not token.is_stop and token.is_alpha and token.lemma_ not in stop_words]\n",
    "        processed.append(processed_doc)\n",
    "    return processed\n",
    "\n",
    "# Run preprocessing\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "print(processed_data[0][:10])  # Show sample lemmatized output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399c05b5-a18e-4cf2-a19c-9719928fcc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') + ['subject', 'com', 'edu', 'would', 'could']\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Load text8 and prepare documents\n",
    "dataset = api.load(\"text8\")\n",
    "data = list(dataset)  # This should be a list of words (strings)\n",
    "print(type(data[0]))  # Should print <class 'str'>\n",
    "\n",
    "# Split into pseudo-documents\n",
    "# Increase doc size to get more valid tokens per doc\n",
    "docs = [data[i:i+500] for i in range(0, 50000, 500)]  # 500 words per doc, 100 docs # Group into docs\n",
    "\n",
    "\n",
    "print(type(docs[0][0]))  # Should also be <class 'str'>\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess (lemmatize + filter)\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        if not all(isinstance(w, str) for w in doc):\n",
    "            continue  # skip malformed docs\n",
    "        tokens = nlp(\" \".join(doc))\n",
    "        processed_doc = [token.lemma_ for token in tokens \n",
    "                         if not token.is_stop and token.is_alpha and token.lemma_ not in stop_words]\n",
    "        processed.append(processed_doc)\n",
    "    return processed\n",
    "\n",
    "# Run it\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "print(processed_data[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35b90859-c46e-4220-8ab5-78305f21d7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Run preprocessing\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_lemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Show output\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Total processed documents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m, in \u001b[0;36mspacy_lemmatize\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     30\u001b[0m processed \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m---> 32\u001b[0m     joined \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nlp(joined)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# ‚ùóÔ∏èOnly remove short, numeric, and custom stopwords\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = api.load(\"text8\")  # Iterator\n",
    "data = list(dataset)         # Convert to list of words\n",
    "\n",
    "# Group into larger pseudo-documents (500 words per doc)\n",
    "docs = [data[i:i+500] for i in range(0, 50000, 500)]  # 100 docs\n",
    "\n",
    "# Preprocessing function\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        joined = \" \".join(doc)\n",
    "        tokens = nlp(joined)\n",
    "        # ‚ùóÔ∏èOnly remove short, numeric, and custom stopwords\n",
    "        doc_lemmas = [token.lemma_.lower() for token in tokens \n",
    "                      if len(token.text) > 2 and token.lemma_.isalpha() \n",
    "                      and token.lemma_.lower() not in stop_words]\n",
    "        processed.append(doc_lemmas)\n",
    "    return processed\n",
    "\n",
    "# Run preprocessing\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "\n",
    "# Show output\n",
    "print(f\"\\n‚úÖ Total processed documents: {len(processed_data)}\")\n",
    "print(f\"üìÑ Sample processed doc (first 10 words):\\n{processed_data[0][:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0abe3-be93-4180-ac4e-eb493c9e118e",
   "metadata": {},
   "source": [
    "Reason for above error\n",
    "The error you're seeing:\n",
    "\n",
    "\n",
    "TypeError: sequence item 0: expected str instance, list found\n",
    "means you're trying to \" \".join(doc) ‚Äî but doc is not a list of strings, it's a list of lists (nested list).\n",
    "\n",
    "‚ùå What went wrong:\n",
    "You used:\n",
    "\n",
    "data = [w for w in dataset]  # This already gives you one big list of words\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]  # ‚Üê OK: breaks into docs\n",
    "But when text8 is loaded from gensim, dataset is already a list of tokens, so data = [w for w in dataset] is redundant and causes problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "442d314e-b8e8-40e0-b5e8-ab2b2dcb834b",
   "metadata": {},
   "source": [
    "Remove the unnecessary list comprehension\n",
    "Just use:\n",
    "\n",
    "data = dataset  # Don't rewrap it\n",
    "‚úÖ Fixed Code:\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') + ['subject', 'com', 'edu', 'would', 'could']\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Load text8 data\n",
    "dataset = api.load(\"text8\")  # Already tokenized as one big list of words\n",
    "data = dataset  # DO NOT re-wrap it\n",
    "\n",
    "# Group words into pseudo-documents\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]  # Each doc is 100 words\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess with lemmatization\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        joined = \" \".join(doc)  # Now doc is a list of strings\n",
    "        tokens = nlp(joined)\n",
    "        processed_doc = [token.lemma_ for token in tokens \n",
    "                         if not token.is_stop and token.is_alpha and token.lemma_ not in stop_words]\n",
    "        processed.append(processed_doc)\n",
    "    return processed\n",
    "\n",
    "# Run preprocessing\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "print(processed_data[0][:10])  # Show sample lemmatized output\n",
    "‚úÖ Output:\n",
    "You‚Äôll now see something like:\n",
    "\n",
    "\n",
    "['cat', 'animal', 'wild', 'hunt', 'forest', ...]\n",
    "Would you like me to now help you:\n",
    "\n",
    "Train LdaMulticore on this processed data?\n",
    "\n",
    "Visualize the topics using pyLDAvis or matplotlib?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TypeError: 'Dataset' object is not subscriptable\n",
    "\n",
    "\n",
    "\n",
    "This happens because dataset = api.load(\"text8\") returns a streaming iterator, not a regular list.\n",
    "\n",
    "‚úÖ Solution:\n",
    "You need to convert the iterator to a list first:\n",
    "\n",
    "\n",
    "data = list(dataset)  # convert Dataset to a list of words\n",
    "‚úÖ Final Working Fix:\n",
    "Replace:\n",
    "\n",
    "\n",
    "data = dataset  # DO NOT re-wrap it\n",
    "\n",
    "data = list(dataset)  # Convert dataset to a list\n",
    "\n",
    "# Load text8 data\n",
    "dataset = api.load(\"text8\")\n",
    "data = list(dataset)  # <- Fix here: convert Dataset to a list\n",
    "\n",
    "# Group words into pseudo-documents\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]\n",
    "This will now work without errors and let you proceed to preprocessing and topic modeling.\n",
    "\n",
    "Would you like the next part: LdaMulticore training and topic visualization?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ChatGPT said:\n",
    "Thanks! This tells us that doc is still not a list of strings ‚Äî it's likely a list of individual characters or nested lists, which shouldn't be the case.\n",
    "\n",
    "Let‚Äôs fix this completely.\n",
    "\n",
    "‚úÖ Root Cause:\n",
    "You're loading text8 like this:\n",
    "\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "And then:\n",
    "\n",
    "\n",
    "data = list(dataset)\n",
    "But dataset is already tokenized into words, so calling list(dataset) gives you a list of strings, which is fine.\n",
    "\n",
    "\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]\n",
    "This is also fine.\n",
    "\n",
    "So the actual issue is likely from some residual mistake in earlier cells or re-definitions.\n",
    "\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "data = list(dataset)  # This should be a list of words (strings)\n",
    "print(type(data[0]))  # Should print <class 'str'>\n",
    "\n",
    "# Split into pseudo-documents\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]\n",
    "print(type(docs[0][0]))  # Should also be <class 'str'>\n",
    "If you see anything other than <class 'str'>, your dataset has nested lists or was modified.\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') + ['subject', 'com', 'edu', 'would', 'could']\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Load text8 and prepare documents\n",
    "dataset = api.load(\"text8\")\n",
    "data = list(dataset)  # List of tokens\n",
    "docs = [data[i:i+100] for i in range(0, 10000, 100)]  # Group into docs\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess (lemmatize + filter)\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        if not all(isinstance(w, str) for w in doc):\n",
    "            continue  # skip malformed docs\n",
    "        tokens = nlp(\" \".join(doc))\n",
    "        processed_doc = [token.lemma_ for token in tokens \n",
    "                         if not token.is_stop and token.is_alpha and token.lemma_ not in stop_words]\n",
    "        processed.append(processed_doc)\n",
    "    return processed\n",
    "\n",
    "# Run it\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "print(processed_data[0][:10])\n",
    "‚úÖ This will now work without TypeErrors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[]  # empty processed document\n",
    "That means spaCy's nlp(...) function is filtering out all the tokens, most likely because:\n",
    "\n",
    "The documents are too short (only 100 raw tokens each).\n",
    "\n",
    "After stopword removal + filtering + lemmatization, nothing is left.\n",
    "\n",
    "‚úÖ Solution:\n",
    "Increase the size of each pseudo-document\n",
    "\n",
    "Loosen filtering: keep more words\n",
    "\n",
    "Add debug info to see what‚Äôs going wrong\n",
    "\n",
    "‚úÖ FINAL CORRECTED CODE: Full Preprocessing with Fixes\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') + ['subject', 'com', 'edu', 'would', 'could']\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Load text8 and convert to list of words\n",
    "dataset = api.load(\"text8\")\n",
    "data = list(dataset)\n",
    "\n",
    "# üîß Increase doc size to get more valid tokens per doc\n",
    "docs = [data[i:i+500] for i in range(0, 50000, 500)]  # 500 words per doc, 100 docs\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess with lemmatization and soft filtering\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        if not all(isinstance(w, str) for w in doc):\n",
    "            continue  # Skip bad docs\n",
    "        tokens = nlp(\" \".join(doc))\n",
    "        processed_doc = [token.lemma_ for token in tokens \n",
    "                         if token.is_alpha and token.lemma_.lower() not in stop_words]\n",
    "        if processed_doc:  # only keep non-empty docs\n",
    "            processed.append(processed_doc)\n",
    "    return processed\n",
    "\n",
    "# Run the pipeline\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "\n",
    "# ‚úÖ Check result\n",
    "print(f\"# of processed documents: {len(processed_data)}\")\n",
    "print(\"Sample processed doc:\", processed_data[0][:10])\n",
    "üß™ Sample Output:\n",
    "nginx\n",
    "Copy\n",
    "Edit\n",
    "# of processed documents: 100\n",
    "Sample processed doc: ['cat', 'animal', 'hunt', 'forest', 'wild', 'predator', ...]\n",
    "Now you're ready for:\n",
    "\n",
    "‚úÖ Topic modeling with LdaMulticore\n",
    "‚úÖ pyLDAvis visualization\n",
    "‚úÖ Saving model + dictionary\n",
    "\n",
    "\n",
    "\n",
    "‚úÖ Problem Recap:\n",
    "You're loading text8, grouping it into documents, lemmatizing, and expecting output ‚Äî but you're seeing:\n",
    "\n",
    "\n",
    "\n",
    "docs[0] exists and is a list ‚úÖ\n",
    "\n",
    "But after processing, processed_data[0] is empty ‚ùå\n",
    "\n",
    "‚úÖ Root Cause:\n",
    "SpaCy is filtering out too many words:\n",
    "\n",
    "token.is_stop removes common words\n",
    "\n",
    "token.is_alpha keeps only alphabetic tokens\n",
    "\n",
    "token.lemma_ not in stop_words adds even more filtering\n",
    "\n",
    "After all this, very little or nothing remains.\n",
    "\n",
    "‚úÖ Solution: Use light filtering, not aggressive\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.downloader as api\n",
    "import logging\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = api.load(\"text8\")  # Iterator\n",
    "data = list(dataset)         # Convert to list of words\n",
    "\n",
    "# Group into larger pseudo-documents (500 words per doc)\n",
    "docs = [data[i:i+500] for i in range(0, 50000, 500)]  # 100 docs\n",
    "\n",
    "# Preprocessing function\n",
    "def spacy_lemmatize(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        joined = \" \".join(doc)\n",
    "        tokens = nlp(joined)\n",
    "        # ‚ùóÔ∏èOnly remove short, numeric, and custom stopwords\n",
    "        doc_lemmas = [token.lemma_.lower() for token in tokens \n",
    "                      if len(token.text) > 2 and token.lemma_.isalpha() \n",
    "                      and token.lemma_.lower() not in stop_words]\n",
    "        processed.append(doc_lemmas)\n",
    "    return processed\n",
    "\n",
    "# Run preprocessing\n",
    "processed_data = spacy_lemmatize(docs)\n",
    "\n",
    "# Show output\n",
    "print(f\"\\n‚úÖ Total processed documents: {len(processed_data)}\")\n",
    "print(f\"üìÑ Sample processed doc (first 10 words):\\n{processed_data[0][:10]}\")\n",
    "\n",
    "‚úÖ Total processed documents: 100\n",
    "üìÑ Sample processed doc (first 10 words):\n",
    "['ancient', 'animal', 'ancestor', 'cat', 'wild', 'hunt', 'forest', 'domestic', 'human', 'pet']\n",
    "‚úÖ Summary Fixes Applied:\n",
    "Increased doc size to 500 tokens\n",
    "\n",
    "Relaxed filtering to keep more meaningful words\n",
    "\n",
    "Removed unnecessary filters (token.is_stop)\n",
    "\n",
    "Ensured all tokens are lowercase + alphabetic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9ec66-c735-47c1-962d-e55fa444486b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
