{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc3b69e-e0b9-4c11-806d-e6f0abcbe038",
   "metadata": {},
   "source": [
    "Generate similar is a open source nlp library used for unsupervised topic modeling\n",
    "\n",
    "Topic modelling is a concept in nlp its to find topics based on article or document.gensim is used in topic modeling.\n",
    "\n",
    "Document is a form of statement or sentence.\n",
    "\n",
    "corpus is a collection of documents,each document is seperated by , and with\"\" as 1 st doc,2nd doc,3rd doc etc.,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485da3b-b44f-441c-8546-90ae3839bbf8",
   "metadata": {},
   "source": [
    "vector is a mathematical representation of a document.for 1 st doc we give some number and that number is a vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9395d-8ea3-4321-a478-1fe9f0dc3e34",
   "metadata": {},
   "source": [
    "Model is an algorithym used for transforming vectors from one representation to another .we are transforming from one vector to another vector.that is called model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc5c0f7-41ee-4011-97f4-18fed07fd8f9",
   "metadata": {},
   "source": [
    "prettyprint is used in gensim its like normal print in gensim we call it as prettyprint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50a512d-1989-441c-a233-84e7bbcf1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"NASA’s Artemis program aims to return humans to the Moon. Scientists believe lunar exploration will pave the way for Mars missions. SpaceX also plays a key role in deep space transport.\",\n",
    "    \"Artificial Intelligence is transforming industries. Machine learning algorithms power recommendation systems, autonomous vehicles, and chatbots. Ethical concerns around bias and data privacy remain important.\",\n",
    "    \"Global temperatures are rising due to greenhouse gas emissions. Renewable energy sources like wind and solar are crucial to reduce dependence on fossil fuels. Sea levels continue to rise annually.\",\n",
    "    \"A balanced diet and regular exercise improve physical and mental health. Doctors recommend at least 150 minutes of moderate activity per week. Hydration and sleep are also essential for well-being.\",\n",
    "    \"Understanding budgeting, saving, and investing is key to financial independence. Compound interest can grow savings over time. Many people now use mobile apps to track expenses and build wealth.\",\n",
    "    \"The history of computing spans from the abacus to quantum computers. Alan Turing is considered a pioneer in computer science. The invention of microprocessors revolutionized the tech industry.\"\n",
    "    \"NASA’s Artemis program aims to return humans to the Moon. Scientists believe lunar exploration will pave the way for Mars missions. SpaceX also plays a key role in deep space transport.\",\n",
    "    \"Artificial Intelligence is transforming industries. Machine learning algorithms power recommendation systems, autonomous vehicles, and chatbots. Ethical concerns around bias and data privacy remain important.\",\n",
    "    \"Global temperatures are rising due to greenhouse gas emissions. Renewable energy sources like wind and solar are crucial to reduce dependence on fossil fuels. Sea levels continue to rise annually.\",\n",
    "    \"A balanced diet and regular exercise improve physical and mental health. Doctors recommend at least 150 minutes of moderate activity per week. Hydration and sleep are also essential for well-being.\",\n",
    "    \"Understanding budgeting, saving, and investing is key to financial independence. Compound interest can grow savings over time. Many people now use mobile apps to track expenses and build wealth.\",\n",
    "    \"The history of computing spans from the abacus to quantum computers. Alan Turing is considered a pioneer in computer science. The invention of microprocessors revolutionized the tech industry.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7453be50-45d5-4f79-958e-ed70f1da7de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nasa’s',\n",
      "  'artemis',\n",
      "  'program',\n",
      "  'aims',\n",
      "  'return',\n",
      "  'humans',\n",
      "  'moon.',\n",
      "  'scientists',\n",
      "  'believe',\n",
      "  'lunar',\n",
      "  'exploration',\n",
      "  'will',\n",
      "  'pave',\n",
      "  'way',\n",
      "  'mars',\n",
      "  'missions.',\n",
      "  'spacex',\n",
      "  'also',\n",
      "  'plays',\n",
      "  'key',\n",
      "  'role',\n",
      "  'deep',\n",
      "  'space',\n",
      "  'transport.'],\n",
      " ['artificial',\n",
      "  'intelligence',\n",
      "  'is',\n",
      "  'transforming',\n",
      "  'industries.',\n",
      "  'machine',\n",
      "  'learning',\n",
      "  'algorithms',\n",
      "  'power',\n",
      "  'recommendation',\n",
      "  'systems,',\n",
      "  'autonomous',\n",
      "  'vehicles,',\n",
      "  'chatbots.',\n",
      "  'ethical',\n",
      "  'concerns',\n",
      "  'around',\n",
      "  'bias',\n",
      "  'data',\n",
      "  'privacy',\n",
      "  'remain',\n",
      "  'important.'],\n",
      " ['global',\n",
      "  'temperatures',\n",
      "  'are',\n",
      "  'rising',\n",
      "  'due',\n",
      "  'greenhouse',\n",
      "  'gas',\n",
      "  'emissions.',\n",
      "  'renewable',\n",
      "  'energy',\n",
      "  'sources',\n",
      "  'like',\n",
      "  'wind',\n",
      "  'solar',\n",
      "  'are',\n",
      "  'crucial',\n",
      "  'reduce',\n",
      "  'dependence',\n",
      "  'on',\n",
      "  'fossil',\n",
      "  'fuels.',\n",
      "  'sea',\n",
      "  'levels',\n",
      "  'continue',\n",
      "  'rise',\n",
      "  'annually.'],\n",
      " ['balanced',\n",
      "  'diet',\n",
      "  'regular',\n",
      "  'exercise',\n",
      "  'improve',\n",
      "  'physical',\n",
      "  'mental',\n",
      "  'health.',\n",
      "  'doctors',\n",
      "  'recommend',\n",
      "  'at',\n",
      "  'least',\n",
      "  '150',\n",
      "  'minutes',\n",
      "  'moderate',\n",
      "  'activity',\n",
      "  'per',\n",
      "  'week.',\n",
      "  'hydration',\n",
      "  'sleep',\n",
      "  'are',\n",
      "  'also',\n",
      "  'essential',\n",
      "  'well-being.'],\n",
      " ['understanding',\n",
      "  'budgeting,',\n",
      "  'saving,',\n",
      "  'investing',\n",
      "  'is',\n",
      "  'key',\n",
      "  'financial',\n",
      "  'independence.',\n",
      "  'compound',\n",
      "  'interest',\n",
      "  'can',\n",
      "  'grow',\n",
      "  'savings',\n",
      "  'over',\n",
      "  'time.',\n",
      "  'many',\n",
      "  'people',\n",
      "  'now',\n",
      "  'use',\n",
      "  'mobile',\n",
      "  'apps',\n",
      "  'track',\n",
      "  'expenses',\n",
      "  'build',\n",
      "  'wealth.'],\n",
      " ['history',\n",
      "  'computing',\n",
      "  'spans',\n",
      "  'from',\n",
      "  'abacus',\n",
      "  'quantum',\n",
      "  'computers.',\n",
      "  'alan',\n",
      "  'turing',\n",
      "  'is',\n",
      "  'considered',\n",
      "  'pioneer',\n",
      "  'computer',\n",
      "  'science.',\n",
      "  'invention',\n",
      "  'microprocessors',\n",
      "  'revolutionized',\n",
      "  'tech',\n",
      "  'industry.nasa’s',\n",
      "  'artemis',\n",
      "  'program',\n",
      "  'aims',\n",
      "  'return',\n",
      "  'humans',\n",
      "  'moon.',\n",
      "  'scientists',\n",
      "  'believe',\n",
      "  'lunar',\n",
      "  'exploration',\n",
      "  'will',\n",
      "  'pave',\n",
      "  'way',\n",
      "  'mars',\n",
      "  'missions.',\n",
      "  'spacex',\n",
      "  'also',\n",
      "  'plays',\n",
      "  'key',\n",
      "  'role',\n",
      "  'deep',\n",
      "  'space',\n",
      "  'transport.'],\n",
      " ['artificial',\n",
      "  'intelligence',\n",
      "  'is',\n",
      "  'transforming',\n",
      "  'industries.',\n",
      "  'machine',\n",
      "  'learning',\n",
      "  'algorithms',\n",
      "  'power',\n",
      "  'recommendation',\n",
      "  'systems,',\n",
      "  'autonomous',\n",
      "  'vehicles,',\n",
      "  'chatbots.',\n",
      "  'ethical',\n",
      "  'concerns',\n",
      "  'around',\n",
      "  'bias',\n",
      "  'data',\n",
      "  'privacy',\n",
      "  'remain',\n",
      "  'important.'],\n",
      " ['global',\n",
      "  'temperatures',\n",
      "  'are',\n",
      "  'rising',\n",
      "  'due',\n",
      "  'greenhouse',\n",
      "  'gas',\n",
      "  'emissions.',\n",
      "  'renewable',\n",
      "  'energy',\n",
      "  'sources',\n",
      "  'like',\n",
      "  'wind',\n",
      "  'solar',\n",
      "  'are',\n",
      "  'crucial',\n",
      "  'reduce',\n",
      "  'dependence',\n",
      "  'on',\n",
      "  'fossil',\n",
      "  'fuels.',\n",
      "  'sea',\n",
      "  'levels',\n",
      "  'continue',\n",
      "  'rise',\n",
      "  'annually.'],\n",
      " ['balanced',\n",
      "  'diet',\n",
      "  'regular',\n",
      "  'exercise',\n",
      "  'improve',\n",
      "  'physical',\n",
      "  'mental',\n",
      "  'health.',\n",
      "  'doctors',\n",
      "  'recommend',\n",
      "  'at',\n",
      "  'least',\n",
      "  '150',\n",
      "  'minutes',\n",
      "  'moderate',\n",
      "  'activity',\n",
      "  'per',\n",
      "  'week.',\n",
      "  'hydration',\n",
      "  'sleep',\n",
      "  'are',\n",
      "  'also',\n",
      "  'essential',\n",
      "  'well-being.'],\n",
      " ['understanding',\n",
      "  'budgeting,',\n",
      "  'saving,',\n",
      "  'investing',\n",
      "  'is',\n",
      "  'key',\n",
      "  'financial',\n",
      "  'independence.',\n",
      "  'compound',\n",
      "  'interest',\n",
      "  'can',\n",
      "  'grow',\n",
      "  'savings',\n",
      "  'over',\n",
      "  'time.',\n",
      "  'many',\n",
      "  'people',\n",
      "  'now',\n",
      "  'use',\n",
      "  'mobile',\n",
      "  'apps',\n",
      "  'track',\n",
      "  'expenses',\n",
      "  'build',\n",
      "  'wealth.'],\n",
      " ['history',\n",
      "  'computing',\n",
      "  'spans',\n",
      "  'from',\n",
      "  'abacus',\n",
      "  'quantum',\n",
      "  'computers.',\n",
      "  'alan',\n",
      "  'turing',\n",
      "  'is',\n",
      "  'considered',\n",
      "  'pioneer',\n",
      "  'computer',\n",
      "  'science.',\n",
      "  'invention',\n",
      "  'microprocessors',\n",
      "  'revolutionized',\n",
      "  'tech',\n",
      "  'industry.']]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Correct stoplist splitting (by whitespace)\n",
    "stoplist = set('for a of the and to in'.split())#set so no duplicates\n",
    "\n",
    "# Create a list of lists with words filtered by stoplist\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in text_corpus\n",
    "]\n",
    "\n",
    "# Pretty print the result\n",
    "pprint.pprint(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391c1261-cfba-4460-a832-2bc3d1d9501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['artemis',\n",
      "  'program',\n",
      "  'aims',\n",
      "  'return',\n",
      "  'humans',\n",
      "  'moon.',\n",
      "  'scientists',\n",
      "  'believe',\n",
      "  'lunar',\n",
      "  'exploration',\n",
      "  'will',\n",
      "  'pave',\n",
      "  'way',\n",
      "  'mars',\n",
      "  'missions.',\n",
      "  'spacex',\n",
      "  'also',\n",
      "  'plays',\n",
      "  'key',\n",
      "  'role',\n",
      "  'deep',\n",
      "  'space',\n",
      "  'transport.'],\n",
      " ['artificial',\n",
      "  'intelligence',\n",
      "  'is',\n",
      "  'transforming',\n",
      "  'industries.',\n",
      "  'machine',\n",
      "  'learning',\n",
      "  'algorithms',\n",
      "  'power',\n",
      "  'recommendation',\n",
      "  'systems,',\n",
      "  'autonomous',\n",
      "  'vehicles,',\n",
      "  'chatbots.',\n",
      "  'ethical',\n",
      "  'concerns',\n",
      "  'around',\n",
      "  'bias',\n",
      "  'data',\n",
      "  'privacy',\n",
      "  'remain',\n",
      "  'important.'],\n",
      " ['global',\n",
      "  'temperatures',\n",
      "  'are',\n",
      "  'rising',\n",
      "  'due',\n",
      "  'greenhouse',\n",
      "  'gas',\n",
      "  'emissions.',\n",
      "  'renewable',\n",
      "  'energy',\n",
      "  'sources',\n",
      "  'like',\n",
      "  'wind',\n",
      "  'solar',\n",
      "  'are',\n",
      "  'crucial',\n",
      "  'reduce',\n",
      "  'dependence',\n",
      "  'on',\n",
      "  'fossil',\n",
      "  'fuels.',\n",
      "  'sea',\n",
      "  'levels',\n",
      "  'continue',\n",
      "  'rise',\n",
      "  'annually.'],\n",
      " ['balanced',\n",
      "  'diet',\n",
      "  'regular',\n",
      "  'exercise',\n",
      "  'improve',\n",
      "  'physical',\n",
      "  'mental',\n",
      "  'health.',\n",
      "  'doctors',\n",
      "  'recommend',\n",
      "  'at',\n",
      "  'least',\n",
      "  '150',\n",
      "  'minutes',\n",
      "  'moderate',\n",
      "  'activity',\n",
      "  'per',\n",
      "  'week.',\n",
      "  'hydration',\n",
      "  'sleep',\n",
      "  'are',\n",
      "  'also',\n",
      "  'essential',\n",
      "  'well-being.'],\n",
      " ['understanding',\n",
      "  'budgeting,',\n",
      "  'saving,',\n",
      "  'investing',\n",
      "  'is',\n",
      "  'key',\n",
      "  'financial',\n",
      "  'independence.',\n",
      "  'compound',\n",
      "  'interest',\n",
      "  'can',\n",
      "  'grow',\n",
      "  'savings',\n",
      "  'over',\n",
      "  'time.',\n",
      "  'many',\n",
      "  'people',\n",
      "  'now',\n",
      "  'use',\n",
      "  'mobile',\n",
      "  'apps',\n",
      "  'track',\n",
      "  'expenses',\n",
      "  'build',\n",
      "  'wealth.'],\n",
      " ['history',\n",
      "  'computing',\n",
      "  'spans',\n",
      "  'from',\n",
      "  'abacus',\n",
      "  'quantum',\n",
      "  'computers.',\n",
      "  'alan',\n",
      "  'turing',\n",
      "  'is',\n",
      "  'considered',\n",
      "  'pioneer',\n",
      "  'computer',\n",
      "  'science.',\n",
      "  'invention',\n",
      "  'microprocessors',\n",
      "  'revolutionized',\n",
      "  'tech',\n",
      "  'artemis',\n",
      "  'program',\n",
      "  'aims',\n",
      "  'return',\n",
      "  'humans',\n",
      "  'moon.',\n",
      "  'scientists',\n",
      "  'believe',\n",
      "  'lunar',\n",
      "  'exploration',\n",
      "  'will',\n",
      "  'pave',\n",
      "  'way',\n",
      "  'mars',\n",
      "  'missions.',\n",
      "  'spacex',\n",
      "  'also',\n",
      "  'plays',\n",
      "  'key',\n",
      "  'role',\n",
      "  'deep',\n",
      "  'space',\n",
      "  'transport.'],\n",
      " ['artificial',\n",
      "  'intelligence',\n",
      "  'is',\n",
      "  'transforming',\n",
      "  'industries.',\n",
      "  'machine',\n",
      "  'learning',\n",
      "  'algorithms',\n",
      "  'power',\n",
      "  'recommendation',\n",
      "  'systems,',\n",
      "  'autonomous',\n",
      "  'vehicles,',\n",
      "  'chatbots.',\n",
      "  'ethical',\n",
      "  'concerns',\n",
      "  'around',\n",
      "  'bias',\n",
      "  'data',\n",
      "  'privacy',\n",
      "  'remain',\n",
      "  'important.'],\n",
      " ['global',\n",
      "  'temperatures',\n",
      "  'are',\n",
      "  'rising',\n",
      "  'due',\n",
      "  'greenhouse',\n",
      "  'gas',\n",
      "  'emissions.',\n",
      "  'renewable',\n",
      "  'energy',\n",
      "  'sources',\n",
      "  'like',\n",
      "  'wind',\n",
      "  'solar',\n",
      "  'are',\n",
      "  'crucial',\n",
      "  'reduce',\n",
      "  'dependence',\n",
      "  'on',\n",
      "  'fossil',\n",
      "  'fuels.',\n",
      "  'sea',\n",
      "  'levels',\n",
      "  'continue',\n",
      "  'rise',\n",
      "  'annually.'],\n",
      " ['balanced',\n",
      "  'diet',\n",
      "  'regular',\n",
      "  'exercise',\n",
      "  'improve',\n",
      "  'physical',\n",
      "  'mental',\n",
      "  'health.',\n",
      "  'doctors',\n",
      "  'recommend',\n",
      "  'at',\n",
      "  'least',\n",
      "  '150',\n",
      "  'minutes',\n",
      "  'moderate',\n",
      "  'activity',\n",
      "  'per',\n",
      "  'week.',\n",
      "  'hydration',\n",
      "  'sleep',\n",
      "  'are',\n",
      "  'also',\n",
      "  'essential',\n",
      "  'well-being.'],\n",
      " ['understanding',\n",
      "  'budgeting,',\n",
      "  'saving,',\n",
      "  'investing',\n",
      "  'is',\n",
      "  'key',\n",
      "  'financial',\n",
      "  'independence.',\n",
      "  'compound',\n",
      "  'interest',\n",
      "  'can',\n",
      "  'grow',\n",
      "  'savings',\n",
      "  'over',\n",
      "  'time.',\n",
      "  'many',\n",
      "  'people',\n",
      "  'now',\n",
      "  'use',\n",
      "  'mobile',\n",
      "  'apps',\n",
      "  'track',\n",
      "  'expenses',\n",
      "  'build',\n",
      "  'wealth.'],\n",
      " ['history',\n",
      "  'computing',\n",
      "  'spans',\n",
      "  'from',\n",
      "  'abacus',\n",
      "  'quantum',\n",
      "  'computers.',\n",
      "  'alan',\n",
      "  'turing',\n",
      "  'is',\n",
      "  'considered',\n",
      "  'pioneer',\n",
      "  'computer',\n",
      "  'science.',\n",
      "  'invention',\n",
      "  'microprocessors',\n",
      "  'revolutionized',\n",
      "  'tech']]\n"
     ]
    }
   ],
   "source": [
    "#now count and word frequencies\n",
    "from collections import defaultdict\n",
    "frequency=defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token]+=1\n",
    "\n",
    "#only keep words that appear more than once\n",
    "processed_corpus=[[token for token in text if frequency[token]>=2]for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829719c-a8f8-4494-bedb-020f4fe4cde0",
   "metadata": {},
   "source": [
    "import defaultdict to keep count of word frequencies .then we instantiate defaultdict into frequency object.texts is the collection of words now where words are tokens or terms for every token in texts we take that text and load it into frequency object.if frequency of token>1 then that token will be taken and stored into processed_corpus above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c080c-cd66-4e0c-8d09-f5252db14699",
   "metadata": {},
   "source": [
    "create dictionary .dictionary is available in corpora during topic modeling we have dictionary  so we can refer dictionaryand convert it into some numbers.for that we import corpora from gensim.within corpora we have dictionaryso in dictionary we pass these processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67068d1-7875-430e-ba58-14e6eef6d79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<132 unique tokens: ['aims', 'also', 'artemis', 'believe', 'deep']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary=corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82be7b02-dd18-4402-b7b6-17d569341e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'150': 70,\n",
      " 'abacus': 115,\n",
      " 'activity': 71,\n",
      " 'aims': 0,\n",
      " 'alan': 116,\n",
      " 'algorithms': 23,\n",
      " 'also': 1,\n",
      " 'annually.': 45,\n",
      " 'apps': 92,\n",
      " 'are': 46,\n",
      " 'around': 24,\n",
      " 'artemis': 2,\n",
      " 'artificial': 25,\n",
      " 'at': 72,\n",
      " 'autonomous': 26,\n",
      " 'balanced': 73,\n",
      " 'believe': 3,\n",
      " 'bias': 27,\n",
      " 'budgeting,': 93,\n",
      " 'build': 94,\n",
      " 'can': 95,\n",
      " 'chatbots.': 28,\n",
      " 'compound': 96,\n",
      " 'computer': 117,\n",
      " 'computers.': 118,\n",
      " 'computing': 119,\n",
      " 'concerns': 29,\n",
      " 'considered': 120,\n",
      " 'continue': 47,\n",
      " 'crucial': 48,\n",
      " 'data': 30,\n",
      " 'deep': 4,\n",
      " 'dependence': 49,\n",
      " 'diet': 74,\n",
      " 'doctors': 75,\n",
      " 'due': 50,\n",
      " 'emissions.': 51,\n",
      " 'energy': 52,\n",
      " 'essential': 76,\n",
      " 'ethical': 31,\n",
      " 'exercise': 77,\n",
      " 'expenses': 97,\n",
      " 'exploration': 5,\n",
      " 'financial': 98,\n",
      " 'fossil': 53,\n",
      " 'from': 121,\n",
      " 'fuels.': 54,\n",
      " 'gas': 55,\n",
      " 'global': 56,\n",
      " 'greenhouse': 57,\n",
      " 'grow': 99,\n",
      " 'health.': 78,\n",
      " 'history': 122,\n",
      " 'humans': 6,\n",
      " 'hydration': 79,\n",
      " 'important.': 32,\n",
      " 'improve': 80,\n",
      " 'independence.': 100,\n",
      " 'industries.': 33,\n",
      " 'intelligence': 34,\n",
      " 'interest': 101,\n",
      " 'invention': 123,\n",
      " 'investing': 102,\n",
      " 'is': 35,\n",
      " 'key': 7,\n",
      " 'learning': 36,\n",
      " 'least': 81,\n",
      " 'levels': 58,\n",
      " 'like': 59,\n",
      " 'lunar': 8,\n",
      " 'machine': 37,\n",
      " 'many': 103,\n",
      " 'mars': 9,\n",
      " 'mental': 82,\n",
      " 'microprocessors': 124,\n",
      " 'minutes': 83,\n",
      " 'missions.': 10,\n",
      " 'mobile': 104,\n",
      " 'moderate': 84,\n",
      " 'moon.': 11,\n",
      " 'now': 105,\n",
      " 'on': 60,\n",
      " 'over': 106,\n",
      " 'pave': 12,\n",
      " 'people': 107,\n",
      " 'per': 85,\n",
      " 'physical': 86,\n",
      " 'pioneer': 125,\n",
      " 'plays': 13,\n",
      " 'power': 38,\n",
      " 'privacy': 39,\n",
      " 'program': 14,\n",
      " 'quantum': 126,\n",
      " 'recommend': 87,\n",
      " 'recommendation': 40,\n",
      " 'reduce': 61,\n",
      " 'regular': 88,\n",
      " 'remain': 41,\n",
      " 'renewable': 62,\n",
      " 'return': 15,\n",
      " 'revolutionized': 127,\n",
      " 'rise': 63,\n",
      " 'rising': 64,\n",
      " 'role': 16,\n",
      " 'saving,': 108,\n",
      " 'savings': 109,\n",
      " 'science.': 128,\n",
      " 'scientists': 17,\n",
      " 'sea': 65,\n",
      " 'sleep': 89,\n",
      " 'solar': 66,\n",
      " 'sources': 67,\n",
      " 'space': 18,\n",
      " 'spacex': 19,\n",
      " 'spans': 129,\n",
      " 'systems,': 42,\n",
      " 'tech': 130,\n",
      " 'temperatures': 68,\n",
      " 'time.': 110,\n",
      " 'track': 111,\n",
      " 'transforming': 43,\n",
      " 'transport.': 20,\n",
      " 'turing': 131,\n",
      " 'understanding': 112,\n",
      " 'use': 113,\n",
      " 'vehicles,': 44,\n",
      " 'way': 21,\n",
      " 'wealth.': 114,\n",
      " 'week.': 90,\n",
      " 'well-being.': 91,\n",
      " 'will': 22,\n",
      " 'wind': 69}\n"
     ]
    }
   ],
   "source": [
    "#vector reprsentation\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eecba7-2abe-4684-b352-88f72e1e88e7",
   "metadata": {},
   "source": [
    "vector representation is from one vector to another vector.we use stored values of 'dictionary' object we take all tokens from dictionary values which contains 132 unique tokens and convert these tokens into id.so use cmd token2id above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f0ce61-72a7-4919-a3bb-7a9104f55c61",
   "metadata": {},
   "source": [
    "in tuple below 1st occurrence is id and 2nd occurrence is count.in above all are assigned with ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de1af9f6-de0e-455b-84bd-01b5ccdd8df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(117, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc=\"human computer interaction\"\n",
    "new_vec=dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd321e-5f59-4c55-abd0-257f13904ec0",
   "metadata": {},
   "source": [
    "1st occurrence is id and 2nd occurrence is count here count is the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0a6ca-13d3-46a8-a771-c470e7894795",
   "metadata": {},
   "source": [
    "doc2bow is document to bag of words.here we used the dictionary which we created.count is how many times the id of word is occurring.here word with id 117 of computer is occuring only one time in new doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15254ee7-30fc-4b27-ae18-341060b8cc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(117, 2)]\n"
     ]
    }
   ],
   "source": [
    "new_doc=\"human computer interaction computer\"\n",
    "new_vec=dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479e7ed-4202-4177-a58c-17fccbee0e65",
   "metadata": {},
   "source": [
    "so dictionary asssigns id values to all tokens and we take that id from dictionaryand then gives new vector that is count or occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26ab9d5-667f-464e-84fd-8a76330274ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1),\n",
      "  (1, 1),\n",
      "  (2, 1),\n",
      "  (3, 1),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (15, 1),\n",
      "  (16, 1),\n",
      "  (17, 1),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 1)],\n",
      " [(23, 1),\n",
      "  (24, 1),\n",
      "  (25, 1),\n",
      "  (26, 1),\n",
      "  (27, 1),\n",
      "  (28, 1),\n",
      "  (29, 1),\n",
      "  (30, 1),\n",
      "  (31, 1),\n",
      "  (32, 1),\n",
      "  (33, 1),\n",
      "  (34, 1),\n",
      "  (35, 1),\n",
      "  (36, 1),\n",
      "  (37, 1),\n",
      "  (38, 1),\n",
      "  (39, 1),\n",
      "  (40, 1),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 1),\n",
      "  (44, 1)],\n",
      " [(45, 1),\n",
      "  (46, 2),\n",
      "  (47, 1),\n",
      "  (48, 1),\n",
      "  (49, 1),\n",
      "  (50, 1),\n",
      "  (51, 1),\n",
      "  (52, 1),\n",
      "  (53, 1),\n",
      "  (54, 1),\n",
      "  (55, 1),\n",
      "  (56, 1),\n",
      "  (57, 1),\n",
      "  (58, 1),\n",
      "  (59, 1),\n",
      "  (60, 1),\n",
      "  (61, 1),\n",
      "  (62, 1),\n",
      "  (63, 1),\n",
      "  (64, 1),\n",
      "  (65, 1),\n",
      "  (66, 1),\n",
      "  (67, 1),\n",
      "  (68, 1),\n",
      "  (69, 1)],\n",
      " [(1, 1),\n",
      "  (46, 1),\n",
      "  (70, 1),\n",
      "  (71, 1),\n",
      "  (72, 1),\n",
      "  (73, 1),\n",
      "  (74, 1),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (77, 1),\n",
      "  (78, 1),\n",
      "  (79, 1),\n",
      "  (80, 1),\n",
      "  (81, 1),\n",
      "  (82, 1),\n",
      "  (83, 1),\n",
      "  (84, 1),\n",
      "  (85, 1),\n",
      "  (86, 1),\n",
      "  (87, 1),\n",
      "  (88, 1),\n",
      "  (89, 1),\n",
      "  (90, 1),\n",
      "  (91, 1)],\n",
      " [(7, 1),\n",
      "  (35, 1),\n",
      "  (92, 1),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 1),\n",
      "  (96, 1),\n",
      "  (97, 1),\n",
      "  (98, 1),\n",
      "  (99, 1),\n",
      "  (100, 1),\n",
      "  (101, 1),\n",
      "  (102, 1),\n",
      "  (103, 1),\n",
      "  (104, 1),\n",
      "  (105, 1),\n",
      "  (106, 1),\n",
      "  (107, 1),\n",
      "  (108, 1),\n",
      "  (109, 1),\n",
      "  (110, 1),\n",
      "  (111, 1),\n",
      "  (112, 1),\n",
      "  (113, 1),\n",
      "  (114, 1)],\n",
      " [(0, 1),\n",
      "  (1, 1),\n",
      "  (2, 1),\n",
      "  (3, 1),\n",
      "  (4, 1),\n",
      "  (5, 1),\n",
      "  (6, 1),\n",
      "  (7, 1),\n",
      "  (8, 1),\n",
      "  (9, 1),\n",
      "  (10, 1),\n",
      "  (11, 1),\n",
      "  (12, 1),\n",
      "  (13, 1),\n",
      "  (14, 1),\n",
      "  (15, 1),\n",
      "  (16, 1),\n",
      "  (17, 1),\n",
      "  (18, 1),\n",
      "  (19, 1),\n",
      "  (20, 1),\n",
      "  (21, 1),\n",
      "  (22, 1),\n",
      "  (35, 1),\n",
      "  (115, 1),\n",
      "  (116, 1),\n",
      "  (117, 1),\n",
      "  (118, 1),\n",
      "  (119, 1),\n",
      "  (120, 1),\n",
      "  (121, 1),\n",
      "  (122, 1),\n",
      "  (123, 1),\n",
      "  (124, 1),\n",
      "  (125, 1),\n",
      "  (126, 1),\n",
      "  (127, 1),\n",
      "  (128, 1),\n",
      "  (129, 1),\n",
      "  (130, 1),\n",
      "  (131, 1)],\n",
      " [(23, 1),\n",
      "  (24, 1),\n",
      "  (25, 1),\n",
      "  (26, 1),\n",
      "  (27, 1),\n",
      "  (28, 1),\n",
      "  (29, 1),\n",
      "  (30, 1),\n",
      "  (31, 1),\n",
      "  (32, 1),\n",
      "  (33, 1),\n",
      "  (34, 1),\n",
      "  (35, 1),\n",
      "  (36, 1),\n",
      "  (37, 1),\n",
      "  (38, 1),\n",
      "  (39, 1),\n",
      "  (40, 1),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 1),\n",
      "  (44, 1)],\n",
      " [(45, 1),\n",
      "  (46, 2),\n",
      "  (47, 1),\n",
      "  (48, 1),\n",
      "  (49, 1),\n",
      "  (50, 1),\n",
      "  (51, 1),\n",
      "  (52, 1),\n",
      "  (53, 1),\n",
      "  (54, 1),\n",
      "  (55, 1),\n",
      "  (56, 1),\n",
      "  (57, 1),\n",
      "  (58, 1),\n",
      "  (59, 1),\n",
      "  (60, 1),\n",
      "  (61, 1),\n",
      "  (62, 1),\n",
      "  (63, 1),\n",
      "  (64, 1),\n",
      "  (65, 1),\n",
      "  (66, 1),\n",
      "  (67, 1),\n",
      "  (68, 1),\n",
      "  (69, 1)],\n",
      " [(1, 1),\n",
      "  (46, 1),\n",
      "  (70, 1),\n",
      "  (71, 1),\n",
      "  (72, 1),\n",
      "  (73, 1),\n",
      "  (74, 1),\n",
      "  (75, 1),\n",
      "  (76, 1),\n",
      "  (77, 1),\n",
      "  (78, 1),\n",
      "  (79, 1),\n",
      "  (80, 1),\n",
      "  (81, 1),\n",
      "  (82, 1),\n",
      "  (83, 1),\n",
      "  (84, 1),\n",
      "  (85, 1),\n",
      "  (86, 1),\n",
      "  (87, 1),\n",
      "  (88, 1),\n",
      "  (89, 1),\n",
      "  (90, 1),\n",
      "  (91, 1)],\n",
      " [(7, 1),\n",
      "  (35, 1),\n",
      "  (92, 1),\n",
      "  (93, 1),\n",
      "  (94, 1),\n",
      "  (95, 1),\n",
      "  (96, 1),\n",
      "  (97, 1),\n",
      "  (98, 1),\n",
      "  (99, 1),\n",
      "  (100, 1),\n",
      "  (101, 1),\n",
      "  (102, 1),\n",
      "  (103, 1),\n",
      "  (104, 1),\n",
      "  (105, 1),\n",
      "  (106, 1),\n",
      "  (107, 1),\n",
      "  (108, 1),\n",
      "  (109, 1),\n",
      "  (110, 1),\n",
      "  (111, 1),\n",
      "  (112, 1),\n",
      "  (113, 1),\n",
      "  (114, 1)],\n",
      " [(35, 1),\n",
      "  (115, 1),\n",
      "  (116, 1),\n",
      "  (117, 1),\n",
      "  (118, 1),\n",
      "  (119, 1),\n",
      "  (120, 1),\n",
      "  (121, 1),\n",
      "  (122, 1),\n",
      "  (123, 1),\n",
      "  (124, 1),\n",
      "  (125, 1),\n",
      "  (126, 1),\n",
      "  (127, 1),\n",
      "  (128, 1),\n",
      "  (129, 1),\n",
      "  (130, 1),\n",
      "  (131, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#now apply dictionary to whole text\n",
    "bow_corpus=[dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9536ef60-ced0-4075-910e-3dae7e67c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26, 0.7071067811865476), (117, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "#hence we got id and no of occurrence or count above\n",
    "from gensim import models\n",
    "#train the model\n",
    "tfidf=models.TfidfModel(bow_corpus)\n",
    "#now transform the \"computer autonomous\" string\n",
    "words=\"computer autonomous\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58888b3-71bf-4abc-9cb8-bc024ec6e87b",
   "metadata": {},
   "source": [
    "we converted words into vector using tfidf model.and this is tfidf vector value.we got id and vector.id we got using token2id and vector we got using doc2bow using count or occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0ce900-21bd-408f-8710-c0458b1c6b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addressed': 52,\n",
      " 'advancement': 75,\n",
      " 'ai': 0,\n",
      " 'algorithms': 12,\n",
      " 'also': 41,\n",
      " 'analytics': 30,\n",
      " 'and': 13,\n",
      " 'are': 14,\n",
      " 'artificial': 1,\n",
      " 'as': 62,\n",
      " 'assist': 15,\n",
      " 'automation': 16,\n",
      " 'be': 53,\n",
      " 'becoming': 22,\n",
      " 'benefits': 63,\n",
      " 'between': 67,\n",
      " 'bias': 42,\n",
      " 'businesses': 31,\n",
      " 'but': 76,\n",
      " 'collaboration': 68,\n",
      " 'concerns': 43,\n",
      " 'continues': 69,\n",
      " 'crucial': 70,\n",
      " 'customer': 32,\n",
      " 'data': 44,\n",
      " 'decisions': 17,\n",
      " 'depends': 77,\n",
      " 'deployed': 78,\n",
      " 'detection': 23,\n",
      " 'disease': 24,\n",
      " 'displacement': 54,\n",
      " 'driving': 25,\n",
      " 'enabling': 18,\n",
      " 'engagement': 33,\n",
      " 'ensure': 64,\n",
      " 'ethical': 45,\n",
      " 'ethicists': 71,\n",
      " 'evolve': 72,\n",
      " 'for': 34,\n",
      " 'forming': 55,\n",
      " 'from': 2,\n",
      " 'future': 79,\n",
      " 'governments': 56,\n",
      " 'healthcare': 3,\n",
      " 'how': 80,\n",
      " 'however': 46,\n",
      " 'important': 47,\n",
      " 'in': 19,\n",
      " 'institutions': 57,\n",
      " 'intelligence': 4,\n",
      " 'is': 5,\n",
      " 'issues': 48,\n",
      " 'it': 81,\n",
      " 'job': 58,\n",
      " 'just': 82,\n",
      " 'like': 49,\n",
      " 'medicine': 26,\n",
      " 'more': 35,\n",
      " 'moreover': 36,\n",
      " 'must': 59,\n",
      " 'not': 83,\n",
      " 'of': 84,\n",
      " 'on': 85,\n",
      " 'personalized': 27,\n",
      " 'policymakers': 73,\n",
      " 'predictive': 37,\n",
      " 'privacy': 50,\n",
      " 'raises': 51,\n",
      " 'rapidly': 6,\n",
      " 'real': 87,\n",
      " 'regulations': 60,\n",
      " 'reliable': 38,\n",
      " 'responsibly': 61,\n",
      " 'scenarios': 88,\n",
      " 'self': 28,\n",
      " 'smarter': 20,\n",
      " 'society': 65,\n",
      " 'technical': 86,\n",
      " 'technologies': 21,\n",
      " 'technologists': 74,\n",
      " 'thanks': 39,\n",
      " 'the': 7,\n",
      " 'to': 8,\n",
      " 'transforming': 9,\n",
      " 'transportation': 10,\n",
      " 'use': 40,\n",
      " 'vehicles': 29,\n",
      " 'whole': 66,\n",
      " 'world': 11}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "import os\n",
    "dict_stf=corpora.Dictionary(simple_preprocess(line) for line in open(r\"sample_data.txt\"))\n",
    "pprint(dict_stf.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1667df5-9df7-4ede-8318-5bc0e41f1d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mycorrectenv]",
   "language": "python",
   "name": "conda-env-mycorrectenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
